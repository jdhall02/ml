{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=orange>Multinomial Naive Bayes for Text Classification</font>\n",
    "#### Introduction to Information Retrieval - Manning\n",
    "url: https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #input\n",
    "import numpy as np #counts\n",
    "#vocabulary, count of tokens\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from math import log # to calculate posterior probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to have max_tokens higher than vocab_size = len(vocabulary), so that we can make use to all the words in the vocabulary for prediction. Otherwise, only the first max_tokens words of the vocabulary will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes dataset - Introduction to Information Retrieval - Manning\n",
    "url: https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "\n",
    "<img src=\"../assets/images/Naive Bayes dataset - Introduction to Information Retrieval - Manning.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = '../input/naive_bayes_demo_data_china/train.csv'\n",
    "f_test = '../input/naive_bayes_demo_data_china/test.csv'\n",
    "f_classes = '../input/naive_bayes_demo_data_china/classes.csv'\n",
    "cols_train = ['class', 'title', 'body']\n",
    "cols_test = ['class', 'title', 'body']\n",
    "cols_classes = ['class_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AG News\n",
    "\n",
    "url: https://github.com/mhjabreel/CharCnn_Keras/tree/master/data/ag_news_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = '../input/ag_news_csv/train.csv'\n",
    "f_test = '../input/ag_news_csv/test.csv'\n",
    "f_classes = '../input/ag_news_csv/classes.txt'\n",
    "cols_train = ['class', 'title', 'body']\n",
    "cols_test = ['class', 'title', 'body']\n",
    "cols_classes = ['class_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f_train,\n",
    "                       header=None)\n",
    "df_train.columns = cols_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(f_test,\n",
    "                      header=None)\n",
    "df_test.columns = cols_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes = pd.read_csv(f_classes,\n",
    "                         header=None)\n",
    "df_classes.columns = cols_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check for data getting loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.head(2))\n",
    "print(df_test.head(2))\n",
    "print(df_classes.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    return (vectorizer, list) the vocabulary of words that occur in the documents\n",
    "    \n",
    "    arguments:\n",
    "    vec: (vectorizer) (of type tensorflow.keras.layers.experimental.preprocessing.TextVectorization)\n",
    "    df_docs: (dataframe) with documents\n",
    "    df_docs_colname: (string) column name of the column having the documents\n",
    "\"\"\"\n",
    "def extract_vocabulary(vec, df_docs, doc_colname='body'):\n",
    "    #WARNING: https://stackoverflow.com/questions/62679020/error-while-trying-to-save-npy-numpy-file\n",
    "    #VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
    "    vec.adapt(df_docs[doc_colname].values)\n",
    "    return vec, vec.get_vocabulary()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return (number) the number of rows in the dataframe, each row corresponding to a document\n",
    "    \n",
    "    arguments:\n",
    "    df_docs: (dataframe) with documents (can have columns in addition to the document)\n",
    "\"\"\"\n",
    "def count_docs(df_docs):\n",
    "    return len(df_docs)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return (number) the number of rows in the dataframe belonging to class 'c'\n",
    "    \n",
    "    arguments:\n",
    "    df_docs: (dataframe) with documents (should have a 'class' column in addition to the document)\n",
    "    c: (int) the class for which the document count is sought\n",
    "    class_colname: (string) column name of the column having class name/id\n",
    "\"\"\"\n",
    "def count_docs_in_class(df_docs, c, class_colname='class'):\n",
    "    return count_docs(df_docs[df_docs[class_colname] == c])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return (string) the concatenated text of all documents of class c\n",
    "    \n",
    "    arguments:\n",
    "    df_docs: (dataframe) with documents (should have a 'class' column in addition to the document)\n",
    "    c: (int) the class for which the documents are sought\n",
    "    df_docs_colname: (string) column name of the column having the documents\n",
    "    class_colname: (string) column name of the column having class name/id\n",
    "\"\"\"\n",
    "def concatenate_text_of_all_docs_in_class(df_docs, c, doc_colname='body', class_colname='class'):\n",
    "    #for df[class == c] get the 'body' column, convert its type to list,\n",
    "    # and concatenate all lists, separated by space.\n",
    "    return ' '.join(df_docs[df_docs[class_colname] == c][doc_colname].to_list())\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "    return (numpy 1-d array) an array of counts of tokens in the text,\n",
    "    each array element corresponding to the count of token at that index in the vocabulary.\n",
    "    \n",
    "    arguments:\n",
    "    vec: (vectorizer) output_mode=\"count\" based vectorizer\n",
    "        that was 'adapt'ed to the documents to learn the vocabulary.\n",
    "    text: (string) concatenated documents belonging to a certain class\n",
    "'''\n",
    "def count_tokens_of_all_terms(vec, text):\n",
    "    counts = vec([[text]])\n",
    "    return counts[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/images/TrainMultinomialNB.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    return \n",
    "            (vectorizer),\n",
    "            \n",
    "            prior: (1-d array) of dim num_classes\n",
    "                   prior probability of document belonging to class,\n",
    "            \n",
    "            cond_prob: (2-d array) of dim token x num_classes\n",
    "                       conditional probability for all token, given the class\n",
    "            \n",
    "    arguments:\n",
    "    df_classes: (dataframe) with class names\n",
    "    df_docs: (dataframe) with documents (should have a 'class' column in addition to the document 'body' column).\n",
    "    doc_colname, class_colname: (string)(string) provide suitable column names otherwise, using the doc_colname and class_colname arguments.\n",
    "'''\n",
    "def train_multinomial_nb(df_classes, df_docs,\n",
    "                         doc_colname='body', class_colname='class'):\n",
    "    \n",
    "    #extract vocabulary\n",
    "    \n",
    "    #create an instance of TextVectorization\n",
    "    vectorizer = TextVectorization(max_tokens=max_tokens, output_mode=\"count\")\n",
    "    vectorizer, vocabulary = extract_vocabulary(vectorizer, df_docs, doc_colname)\n",
    "    \n",
    "    #data structures for prior and conditional probabilities\n",
    "    \n",
    "    num_classes = len(df_classes) #number of classes of documents\n",
    "    prior = np.zeros((num_classes), dtype=float) #prior probability of document belonging to a class\n",
    "    vocab_size = len(vocabulary)\n",
    "    cond_prob = np.zeros((vocab_size, num_classes), dtype=float)\n",
    "\n",
    "    #total number of documents, to calculate prior probability of document belonging to a class\n",
    "    N = count_docs(df_docs)\n",
    "    \n",
    "    #for each class...\n",
    "    for c in range(num_classes):\n",
    "        cls = c+1 #c is 0-based index, but class is 1-based index\n",
    "\n",
    "        #calculate prior probability of document belonging to class cls\n",
    "        N_c = count_docs_in_class(df_docs, cls, class_colname) #number of document of class cls\n",
    "        prior[c] = N_c / N # (num_docs of class cls / total num_docs)\n",
    "\n",
    "        #calculate likelihood: conditional probability for all token, given this class \n",
    "\n",
    "        #1. concatenate documents of this class\n",
    "        text_c = concatenate_text_of_all_docs_in_class(df_docs, cls, doc_colname, class_colname)\n",
    "        #2. in this concatenated text, get counts for each token\n",
    "        T_ct_all_t = count_tokens_of_all_terms(vectorizer, text_c)\n",
    "        #3. calculate denominator of conditional probability: sigma(T_ct) + |V|\n",
    "        den = T_ct_all_t.sum() + vocab_size\n",
    "        #4 set the likelihood: conditional probability for all tokens, given this class\n",
    "        for t in range(vocab_size):\n",
    "            t_idx = t+1 #vectorizer begins with an OOV, so, add 1 to accommodate OOV\n",
    "            cond_prob[t][c] = (T_ct_all_t[t_idx] + 1)/den\n",
    "            \n",
    "    return vectorizer, prior, cond_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/images/ApplyMultinomialNB.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    return (integer) the (0-based) index of class to which the document belongs\n",
    "    \n",
    "    arguments:\n",
    "    df_classes: (dataframe) with class names\n",
    "    (vectorizer)\n",
    "    prior: (1-d array) of dim num_classes (prior probability of document belonging to class)\n",
    "    cond_prob: (2-d array) of dim token x num_classes (conditional probability for all token, given the class)\n",
    "    text: (string) document whose class is to be predicted\n",
    "'''\n",
    "def apply_multinomial_nb(df_classes, vectorizer, prior, cond_prob, text):\n",
    "    #count of each token\n",
    "    W = count_tokens_of_all_terms(vectorizer, text)\n",
    "\n",
    "    #vocabulary\n",
    "    vocabulary = vectorizer.get_vocabulary()\n",
    "    vocab_size = len(vocabulary)\n",
    "\n",
    "    num_classes = len(df_classes) #number of classes of documents\n",
    "    score = np.zeros((num_classes), dtype=float)\n",
    "\n",
    "    #for each class, calculate the posterior probability\n",
    "\n",
    "    #for each class...\n",
    "    for c in range(num_classes):\n",
    "\n",
    "        #for this class, add the log-prior probability to the score\n",
    "        score[c] += log(prior[c], 10) #log to the base 10\n",
    "\n",
    "        #for each term, add the log-likelihood to the score\n",
    "\n",
    "        for t in range(vocab_size):\n",
    "            t_idx = t+1 #vectorizer begins with an OOV, so, add 1 to accommodate OOV\n",
    "            T_ct = W[t_idx] #count of number of times this token appeared in text\n",
    "\n",
    "            #for a term appearing multiple times, repeatedly add the log-likelihood to the score\n",
    "            for j in range(T_ct):\n",
    "                score[c] += log(cond_prob[t][c], 10) #log to the base 10\n",
    "                \n",
    "    #return the index of class with the maximum-a-posterior probability\n",
    "    return score.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "vectorizer, prior, cond_prob = train_multinomial_nb(df_classes, df_train)\n",
    "print('vocabulary length is:', len(vectorizer.get_vocabulary()))\n",
    "print('vocabulary is:', vectorizer.get_vocabulary()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the documents\n",
    "N = len(df_test)\n",
    "count_correct, count_incorrect = 0, 0\n",
    "for i in range(1000):\n",
    "    text = df_test['body'][i] #input document for classification\n",
    "    pred_cls = apply_multinomial_nb(df_classes, vectorizer, prior, cond_prob, text)\n",
    "    actual_cls = df_test['class'][i] - 1 #df_test.class is 1-based index, but df_classes and pred_class are 0-based index\n",
    "    #print('(predicted, actual):', df_classes['class_name'][pred_cls], df_classes['class_name'][actual_cls])\n",
    "    if pred_cls == actual_cls:\n",
    "        count_correct += 1\n",
    "    else:\n",
    "        count_incorrect += 1\n",
    "print('Correct: ', count_correct, 'Incorrect: ', count_incorrect)\n",
    "print('Percentage of correct predictions: ', (count_correct * 100)/(count_correct + count_incorrect))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
